18/02/27 19:56:46 DEBUG EXECUTOR-LOG:: START EXECUTOR DEBUG LOG LEVEL
18/02/27 19:56:46 DEBUG EXECUTOR-LOG:: START EXECUTOR DEBUG LOG LEVEL
18/02/27 19:56:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1012 bytes result sent to driver
18/02/27 19:56:46 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1012 bytes result sent to driver
18/02/27 19:56:46 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1012 bytes result sent to driver
18/02/27 19:56:46 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 2
18/02/27 19:56:46 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
18/02/27 19:56:46 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 1
18/02/27 19:56:46 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0
18/02/27 19:56:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 182 ms on localhost (executor driver) (1/3)
18/02/27 19:56:46 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 159 ms on localhost (executor driver) (2/3)
18/02/27 19:56:46 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 161 ms on localhost (executor driver) (3/3)
18/02/27 19:56:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/02/27 19:56:46 INFO DAGScheduler: ResultStage 0 (foreachPartition at <console>:25) finished in 0.202 s
18/02/27 19:56:46 DEBUG DAGScheduler: After removal of stage 0, remaining stages = 0
18/02/27 19:56:46 INFO DAGScheduler: Job 0 finished: foreachPartition at <console>:25, took 0.317709 s                                                                              
18/02/27 19:57:03 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(0)
18/02/27 19:57:03 DEBUG ContextCleaner: Cleaning broadcast 0
18/02/27 19:57:03 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 0
18/02/27 19:57:03 DEBUG BlockManagerSlaveEndpoint: removing broadcast 0
18/02/27 19:57:03 DEBUG BlockManager: Removing broadcast 0
18/02/27 19:57:03 DEBUG BlockManager: Removing block broadcast_0_piece0
18/02/27 19:57:03 DEBUG MemoryStore: Block broadcast_0_piece0 of size 943 dropped from memory (free 384092068)
18/02/27 19:57:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.157.131:42721 in memory (size: 943.0 B, free: 366.3 MB)
18/02/27 19:57:03 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0
18/02/27 19:57:03 DEBUG BlockManager: Told master about block broadcast_0_piece0
18/02/27 19:57:03 DEBUG BlockManager: Removing block broadcast_0
18/02/27 19:57:03 DEBUG MemoryStore: Block broadcast_0 of size 1320 dropped from memory (free 384093388)
18/02/27 19:57:03 DEBUG BlockManagerSlaveEndpoint: Done removing broadcast 0, response is 0
18/02/27 19:57:03 DEBUG ContextCleaner: Cleaned broadcast 0
18/02/27 19:57:03 DEBUG BlockManagerSlaveEndpoint: Sent response: 0 to 192.168.157.131:39533
18/02/27 19:57:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 236.5 KB, free 366.1 MB)
18/02/27 19:57:03 DEBUG BlockManager: Put block broadcast_1 locally took  10 ms
18/02/27 19:57:03 DEBUG BlockManager: Putting block broadcast_1 without replication took  11 ms
18/02/27 19:57:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.0 MB)
18/02/27 19:57:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.157.131:42721 (size: 22.9 KB, free: 366.3 MB)
18/02/27 19:57:03 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0
18/02/27 19:57:03 DEBUG BlockManager: Told master about block broadcast_1_piece0
18/02/27 19:57:03 DEBUG BlockManager: Put block broadcast_1_piece0 locally took  4 ms
18/02/27 19:57:03 DEBUG BlockManager: Putting block broadcast_1_piece0 without replication took  4 ms
18/02/27 19:57:03 INFO SparkContext: Created broadcast 1 from textFile at <console>:24
18/02/27 19:57:03 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$30) +++
18/02/27 19:57:03 DEBUG ClosureCleaner:  + declared fields: 2
18/02/27 19:57:03 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$30.serialVersionUID
18/02/27 19:57:03 DEBUG ClosureCleaner:      private final org.apache.spark.SparkContext$$anonfun$hadoopFile$1 org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$30.$outer
18/02/27 19:57:03 DEBUG ClosureCleaner:  + declared methods: 2
18/02/27 19:57:03 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$30.apply(java.lang.Object)
18/02/27 19:57:03 DEBUG ClosureCleaner:      public final void org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$30.apply(org.apache.hadoop.mapred.JobConf)
18/02/27 19:57:03 DEBUG ClosureCleaner:  + inner classes: 0
18/02/27 19:57:03 DEBUG ClosureCleaner:  + outer classes: 2
18/02/27 19:57:03 DEBUG ClosureCleaner:      org.apache.spark.SparkContext$$anonfun$hadoopFile$1
18/02/27 19:57:03 DEBUG ClosureCleaner:      org.apache.spark.SparkContext
18/02/27 19:57:03 DEBUG ClosureCleaner:  + outer objects: 2
18/02/27 19:57:03 DEBUG ClosureCleaner:      <function0>
18/02/27 19:57:03 DEBUG ClosureCleaner:      org.apache.spark.SparkContext@2b27d5d3
18/02/27 19:57:03 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
18/02/27 19:57:03 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
18/02/27 19:57:03 DEBUG ClosureCleaner:      (class org.apache.spark.SparkContext$$anonfun$hadoopFile$1,Set(path$6))
18/02/27 19:57:03 DEBUG ClosureCleaner:      (class org.apache.spark.SparkContext,Set())
18/02/27 19:57:03 DEBUG ClosureCleaner:  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.SparkContext,org.apache.spark.SparkContext@2b27d5d3)
18/02/27 19:57:03 DEBUG ClosureCleaner:  + cloning the object <function0> of class org.apache.spark.SparkContext$$anonfun$hadoopFile$1
18/02/27 19:57:03 DEBUG ClosureCleaner:  + cleaning cloned closure <function0> recursively (org.apache.spark.SparkContext$$anonfun$hadoopFile$1)
18/02/27 19:57:03 DEBUG ClosureCleaner: +++ Cleaning closure <function0> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1) +++
18/02/27 19:57:03 DEBUG ClosureCleaner:  + declared fields: 7
18/02/27 19:57:03 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$hadoopFile$1.serialVersionUID
18/02/27 19:57:03 DEBUG ClosureCleaner:      private final org.apache.spark.SparkContext org.apache.spark.SparkContext$$anonfun$hadoopFile$1.$outer
18/02/27 19:57:03 DEBUG ClosureCleaner:      public final java.lang.String org.apache.spark.SparkContext$$anonfun$hadoopFile$1.path$6
18/02/27 19:57:03 DEBUG ClosureCleaner:      private final java.lang.Class org.apache.spark.SparkContext$$anonfun$hadoopFile$1.inputFormatClass$1
18/02/27 19:57:03 DEBUG ClosureCleaner:      private final java.lang.Class org.apache.spark.SparkContext$$anonfun$hadoopFile$1.keyClass$1
18/02/27 19:57:03 DEBUG ClosureCleaner:      private final java.lang.Class org.apache.spark.SparkContext$$anonfun$hadoopFile$1.valueClass$1
18/02/27 19:57:03 DEBUG ClosureCleaner:      private final int org.apache.spark.SparkContext$$anonfun$hadoopFile$1.minPartitions$3
18/02/27 19:57:03 DEBUG ClosureCleaner:  + declared methods: 2
18/02/27 19:57:03 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply()
18/02/27 19:57:03 DEBUG ClosureCleaner:      public final org.apache.spark.rdd.HadoopRDD org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply()
18/02/27 19:57:03 DEBUG ClosureCleaner:  + inner classes: 1
18/02/27 19:57:03 DEBUG ClosureCleaner:      org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$30
18/02/27 19:57:03 DEBUG ClosureCleaner:  + outer classes: 1
18/02/27 19:57:03 DEBUG ClosureCleaner:      org.apache.spark.SparkContext
18/02/27 19:57:03 DEBUG ClosureCleaner:  + outer objects: 1
18/02/27 19:57:03 DEBUG ClosureCleaner:      org.apache.spark.SparkContext@2b27d5d3
18/02/27 19:57:03 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
18/02/27 19:57:03 DEBUG ClosureCleaner:      (class org.apache.spark.SparkContext$$anonfun$hadoopFile$1,Set(path$6))
18/02/27 19:57:03 DEBUG ClosureCleaner:      (class org.apache.spark.SparkContext,Set())
18/02/27 19:57:03 DEBUG ClosureCleaner:  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.SparkContext,org.apache.spark.SparkContext@2b27d5d3)
18/02/27 19:57:03 DEBUG ClosureCleaner:  + the starting closure doesn't actually need org.apache.spark.SparkContext@2b27d5d3, so we null it out
18/02/27 19:57:03 DEBUG ClosureCleaner:  +++ closure <function0> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1) is now cleaned +++
18/02/27 19:57:03 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$30) is now cleaned +++
18/02/27 19:57:03 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8) +++
18/02/27 19:57:03 DEBUG ClosureCleaner:  + declared fields: 1
18/02/27 19:57:03 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.serialVersionUID
18/02/27 19:57:03 DEBUG ClosureCleaner:  + declared methods: 2
18/02/27 19:57:03 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(java.lang.Object)
18/02/27 19:57:03 DEBUG ClosureCleaner:      public final java.lang.String org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(scala.Tuple2)
18/02/27 19:57:03 DEBUG ClosureCleaner:  + inner classes: 0
18/02/27 19:57:03 DEBUG ClosureCleaner:  + outer classes: 0
18/02/27 19:57:03 DEBUG ClosureCleaner:  + outer objects: 0
18/02/27 19:57:03 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
18/02/27 19:57:03 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
18/02/27 19:57:03 DEBUG ClosureCleaner:  + there are no enclosing objects!
18/02/27 19:57:03 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8) is now cleaned +++
textfile: org.apache.spark.rdd.RDD[String] = /home/kopps/Desktop/phase1/final_extraction.txt MapPartitionsRDD[2] at textFile at <console>:24
18/02/27 19:57:23 DEBUG ClosureCleaner: +++ Cleaning closure <function1> ($line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2) +++
18/02/27 19:57:23 DEBUG ClosureCleaner:  + declared fields: 1
18/02/27 19:57:23 DEBUG ClosureCleaner:      public static final long $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.serialVersionUID
18/02/27 19:57:23 DEBUG ClosureCleaner:  + declared methods: 2
18/02/27 19:57:23 DEBUG ClosureCleaner:      public final java.lang.Object $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(java.lang.Object)
18/02/27 19:57:23 DEBUG ClosureCleaner:      public final scala.collection.mutable.ArrayOps $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(java.lang.String)
18/02/27 19:57:23 DEBUG ClosureCleaner:  + inner classes: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + outer classes: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + outer objects: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
18/02/27 19:57:23 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + there are no enclosing objects!
18/02/27 19:57:23 DEBUG ClosureCleaner:  +++ closure <function1> ($line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2) is now cleaned +++
18/02/27 19:57:23 DEBUG ClosureCleaner: +++ Cleaning closure <function1> ($line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$3) +++
18/02/27 19:57:23 DEBUG ClosureCleaner:  + declared fields: 1
18/02/27 19:57:23 DEBUG ClosureCleaner:      public static final long $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$3.serialVersionUID
18/02/27 19:57:23 DEBUG ClosureCleaner:  + declared methods: 2
18/02/27 19:57:23 DEBUG ClosureCleaner:      public final java.lang.Object $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$3.apply(java.lang.Object)
18/02/27 19:57:23 DEBUG ClosureCleaner:      public final scala.Tuple2 $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$3.apply(java.lang.String)
18/02/27 19:57:23 DEBUG ClosureCleaner:  + inner classes: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + outer classes: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + outer objects: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
18/02/27 19:57:23 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + there are no enclosing objects!
18/02/27 19:57:23 DEBUG ClosureCleaner:  +++ closure <function1> ($line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$3) is now cleaned +++
18/02/27 19:57:23 DEBUG BlockManager: Getting local block broadcast_1
18/02/27 19:57:23 DEBUG BlockManager: Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
18/02/27 19:57:23 DEBUG HadoopRDD: Creating new JobConf and caching it for later re-use
18/02/27 19:57:23 DEBUG FileInputFormat: Time taken to get FileStatuses: 2
18/02/27 19:57:23 INFO FileInputFormat: Total input paths to process : 1
18/02/27 19:57:23 DEBUG FileInputFormat: Total # of splits generated by getSplits: 2, TimeTaken: 8
18/02/27 19:57:23 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$1$$anonfun$apply$16) +++
18/02/27 19:57:23 DEBUG ClosureCleaner:  + declared fields: 1
18/02/27 19:57:23 DEBUG ClosureCleaner:      public static final long org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$1$$anonfun$apply$16.serialVersionUID
18/02/27 19:57:23 DEBUG ClosureCleaner:  + declared methods: 1
18/02/27 19:57:23 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$1$$anonfun$apply$16.apply(java.lang.Object)
18/02/27 19:57:23 DEBUG ClosureCleaner:  + inner classes: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + outer classes: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + outer objects: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
18/02/27 19:57:23 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + there are no enclosing objects!
18/02/27 19:57:23 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$1$$anonfun$apply$16) is now cleaned +++
18/02/27 19:57:23 DEBUG ClosureCleaner: +++ Cleaning closure <function2> ($line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1) +++
18/02/27 19:57:23 DEBUG ClosureCleaner:  + declared fields: 1
18/02/27 19:57:23 DEBUG ClosureCleaner:      public static final long $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.serialVersionUID
18/02/27 19:57:23 DEBUG ClosureCleaner:  + declared methods: 3
18/02/27 19:57:23 DEBUG ClosureCleaner:      public int $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcIII$sp(int,int)
18/02/27 19:57:23 DEBUG ClosureCleaner:      public final int $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(int,int)
18/02/27 19:57:23 DEBUG ClosureCleaner:      public final java.lang.Object $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(java.lang.Object,java.lang.Object)
18/02/27 19:57:23 DEBUG ClosureCleaner:  + inner classes: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + outer classes: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + outer objects: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
18/02/27 19:57:23 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + there are no enclosing objects!
18/02/27 19:57:23 DEBUG ClosureCleaner:  +++ closure <function2> ($line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1) is now cleaned +++
18/02/27 19:57:23 DEBUG ClosureCleaner: +++ Cleaning closure <function2> ($line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1) +++
18/02/27 19:57:23 DEBUG ClosureCleaner:  + declared fields: 1
18/02/27 19:57:23 DEBUG ClosureCleaner:      public static final long $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.serialVersionUID
18/02/27 19:57:23 DEBUG ClosureCleaner:  + declared methods: 3
18/02/27 19:57:23 DEBUG ClosureCleaner:      public int $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcIII$sp(int,int)
18/02/27 19:57:23 DEBUG ClosureCleaner:      public final int $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(int,int)
18/02/27 19:57:23 DEBUG ClosureCleaner:      public final java.lang.Object $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(java.lang.Object,java.lang.Object)
18/02/27 19:57:23 DEBUG ClosureCleaner:  + inner classes: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + outer classes: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + outer objects: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
18/02/27 19:57:23 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
18/02/27 19:57:23 DEBUG ClosureCleaner:  + there are no enclosing objects!
18/02/27 19:57:23 DEBUG ClosureCleaner:  +++ closure <function2> ($line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1) is now cleaned +++
counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[5] at reduceByKey at <console>:26
18/02/27 19:57:41 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13) +++
18/02/27 19:57:41 DEBUG ClosureCleaner:  + declared fields: 2
18/02/27 19:57:41 DEBUG ClosureCleaner:      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.serialVersionUID
18/02/27 19:57:41 DEBUG ClosureCleaner:      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.$outer
18/02/27 19:57:41 DEBUG ClosureCleaner:  + declared methods: 2
18/02/27 19:57:41 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(java.lang.Object)
18/02/27 19:57:41 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(scala.collection.Iterator)
18/02/27 19:57:41 DEBUG ClosureCleaner:  + inner classes: 0
18/02/27 19:57:41 DEBUG ClosureCleaner:  + outer classes: 2
18/02/27 19:57:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD$$anonfun$collect$1
18/02/27 19:57:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD
18/02/27 19:57:41 DEBUG ClosureCleaner:  + outer objects: 2
18/02/27 19:57:41 DEBUG ClosureCleaner:      <function0>
18/02/27 19:57:41 DEBUG ClosureCleaner:      ShuffledRDD[5] at reduceByKey at <console>:26
18/02/27 19:57:41 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
18/02/27 19:57:41 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
18/02/27 19:57:41 DEBUG ClosureCleaner:      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
18/02/27 19:57:41 DEBUG ClosureCleaner:      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
18/02/27 19:57:41 DEBUG ClosureCleaner:  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,ShuffledRDD[5] at reduceByKey at <console>:26)
18/02/27 19:57:41 DEBUG ClosureCleaner:  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
18/02/27 19:57:41 DEBUG ClosureCleaner:  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
18/02/27 19:57:41 DEBUG ClosureCleaner: +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
18/02/27 19:57:41 DEBUG ClosureCleaner:  + declared fields: 2
18/02/27 19:57:41 DEBUG ClosureCleaner:      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
18/02/27 19:57:41 DEBUG ClosureCleaner:      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
18/02/27 19:57:41 DEBUG ClosureCleaner:  + declared methods: 2
18/02/27 19:57:41 DEBUG ClosureCleaner:      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$outer()
18/02/27 19:57:41 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
18/02/27 19:57:41 DEBUG ClosureCleaner:  + inner classes: 1
18/02/27 19:57:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13
18/02/27 19:57:41 DEBUG ClosureCleaner:  + outer classes: 1
18/02/27 19:57:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD
18/02/27 19:57:41 DEBUG ClosureCleaner:  + outer objects: 1
18/02/27 19:57:41 DEBUG ClosureCleaner:      ShuffledRDD[5] at reduceByKey at <console>:26
18/02/27 19:57:41 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
18/02/27 19:57:41 DEBUG ClosureCleaner:      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
18/02/27 19:57:41 DEBUG ClosureCleaner:      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
18/02/27 19:57:41 DEBUG ClosureCleaner:  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.RDD,ShuffledRDD[5] at reduceByKey at <console>:26)
18/02/27 19:57:41 DEBUG ClosureCleaner:  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
18/02/27 19:57:41 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13) is now cleaned +++
18/02/27 19:57:41 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
18/02/27 19:57:41 DEBUG ClosureCleaner:  + declared fields: 2
18/02/27 19:57:41 DEBUG ClosureCleaner:      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
18/02/27 19:57:41 DEBUG ClosureCleaner:      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
18/02/27 19:57:41 DEBUG ClosureCleaner:  + declared methods: 2
18/02/27 19:57:41 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
18/02/27 19:57:41 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
18/02/27 19:57:41 DEBUG ClosureCleaner:  + inner classes: 0
18/02/27 19:57:41 DEBUG ClosureCleaner:  + outer classes: 0
18/02/27 19:57:41 DEBUG ClosureCleaner:  + outer objects: 0
18/02/27 19:57:41 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
18/02/27 19:57:41 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
18/02/27 19:57:41 DEBUG ClosureCleaner:  + there are no enclosing objects!
18/02/27 19:57:41 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
18/02/27 19:57:41 INFO SparkContext: Starting job: collect at <console>:29
18/02/27 19:57:41 DEBUG SortShuffleManager: Can't use serialized shuffle for shuffle 0 because an aggregator is defined
18/02/27 19:57:41 INFO DAGScheduler: Registering RDD 4 (map at <console>:26)
18/02/27 19:57:41 INFO DAGScheduler: Got job 1 (collect at <console>:29) with 2 output partitions
18/02/27 19:57:41 INFO DAGScheduler: Final stage: ResultStage 2 (collect at <console>:29)
18/02/27 19:57:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
18/02/27 19:57:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
18/02/27 19:57:41 DEBUG DAGScheduler: submitStage(ResultStage 2)
18/02/27 19:57:41 DEBUG DAGScheduler: missing: List(ShuffleMapStage 1)
18/02/27 19:57:41 DEBUG DAGScheduler: submitStage(ShuffleMapStage 1)
18/02/27 19:57:41 DEBUG DAGScheduler: missing: List()
18/02/27 19:57:41 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[4] at map at <console>:26), which has no missing parents
18/02/27 19:57:41 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 1)
18/02/27 19:57:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 366.0 MB)
18/02/27 19:57:41 DEBUG BlockManager: Put block broadcast_2 locally took  0 ms
18/02/27 19:57:41 DEBUG BlockManager: Putting block broadcast_2 without replication took  0 ms
18/02/27 19:57:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 366.0 MB)
18/02/27 19:57:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.157.131:42721 (size: 2.8 KB, free: 366.3 MB)
18/02/27 19:57:41 DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0
18/02/27 19:57:41 DEBUG BlockManager: Told master about block broadcast_2_piece0
18/02/27 19:57:41 DEBUG BlockManager: Put block broadcast_2_piece0 locally took  2 ms
18/02/27 19:57:41 DEBUG BlockManager: Putting block broadcast_2_piece0 without replication took  2 ms
18/02/27 19:57:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
18/02/27 19:57:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[4] at map at <console>:26)
18/02/27 19:57:41 DEBUG DAGScheduler: New pending partitions: Set(0, 1)
18/02/27 19:57:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
18/02/27 19:57:41 DEBUG TaskSetManager: Epoch for TaskSet 1.0: 0
18/02/27 19:57:41 DEBUG TaskSetManager: Valid locality levels for TaskSet 1.0: NO_PREF, ANY
18/02/27 19:57:41 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 0
18/02/27 19:57:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 5989 bytes)
18/02/27 19:57:41 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 5989 bytes)
18/02/27 19:57:41 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
18/02/27 19:57:41 INFO Executor: Running task 0.0 in stage 1.0 (TID 3)
18/02/27 19:57:41 INFO Executor: Running task 1.0 in stage 1.0 (TID 4)
18/02/27 19:57:41 DEBUG Executor: Task 3's epoch is 0
18/02/27 19:57:41 DEBUG BlockManager: Getting local block broadcast_2
18/02/27 19:57:41 DEBUG BlockManager: Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
18/02/27 19:57:41 DEBUG Executor: Task 4's epoch is 0
18/02/27 19:57:41 DEBUG BlockManager: Getting local block broadcast_2
18/02/27 19:57:41 DEBUG BlockManager: Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
18/02/27 19:57:41 INFO HadoopRDD: Input split: file:/home/kopps/Desktop/phase1/final_extraction.txt:41213+41214
18/02/27 19:57:41 DEBUG BlockManager: Getting local block broadcast_1
18/02/27 19:57:41 DEBUG BlockManager: Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
18/02/27 19:57:41 DEBUG HadoopRDD: Re-using cached JobConf
18/02/27 19:57:41 INFO HadoopRDD: Input split: file:/home/kopps/Desktop/phase1/final_extraction.txt:0+41213
18/02/27 19:57:41 DEBUG BlockManager: Getting local block broadcast_1
18/02/27 19:57:41 DEBUG BlockManager: Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
18/02/27 19:57:41 DEBUG HadoopRDD: Re-using cached JobConf
18/02/27 19:57:41 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
18/02/27 19:57:41 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
18/02/27 19:57:41 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
18/02/27 19:57:41 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
18/02/27 19:57:41 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
18/02/27 19:57:41 DEBUG TaskMemoryManager: Task 4 release 0.0 B from org.apache.spark.util.collection.ExternalSorter@50e0330d
18/02/27 19:57:41 DEBUG TaskMemoryManager: Task 3 release 0.0 B from org.apache.spark.util.collection.ExternalSorter@6260e5cd
18/02/27 19:57:41 INFO Executor: Finished task 1.0 in stage 1.0 (TID 4). 1748 bytes result sent to driver
18/02/27 19:57:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 3). 1748 bytes result sent to driver
18/02/27 19:57:41 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 1
18/02/27 19:57:41 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 0
18/02/27 19:57:41 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 331 ms on localhost (executor driver) (1/2)
18/02/27 19:57:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 355 ms on localhost (executor driver) (2/2)
18/02/27 19:57:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/02/27 19:57:41 DEBUG DAGScheduler: ShuffleMapTask finished on driver
18/02/27 19:57:41 DEBUG DAGScheduler: ShuffleMapTask finished on driver
18/02/27 19:57:41 INFO DAGScheduler: ShuffleMapStage 1 (map at <console>:26) finished in 0.356 s
18/02/27 19:57:41 INFO DAGScheduler: looking for newly runnable stages
18/02/27 19:57:41 INFO DAGScheduler: running: Set()
18/02/27 19:57:41 INFO DAGScheduler: waiting: Set(ResultStage 2)
18/02/27 19:57:41 INFO DAGScheduler: failed: Set()
18/02/27 19:57:41 DEBUG MapOutputTrackerMaster: Increasing epoch to 1
18/02/27 19:57:41 DEBUG DAGScheduler: submitStage(ResultStage 2)
18/02/27 19:57:41 DEBUG DAGScheduler: missing: List()
18/02/27 19:57:41 INFO DAGScheduler: Submitting ResultStage 2 (ShuffledRDD[5] at reduceByKey at <console>:26), which has no missing parents
18/02/27 19:57:41 DEBUG DAGScheduler: submitMissingTasks(ResultStage 2)
18/02/27 19:57:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.2 KB, free 366.0 MB)
18/02/27 19:57:41 DEBUG BlockManager: Put block broadcast_3 locally took  1 ms
18/02/27 19:57:41 DEBUG BlockManager: Putting block broadcast_3 without replication took  1 ms
18/02/27 19:57:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1978.0 B, free 366.0 MB)
18/02/27 19:57:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.157.131:42721 (size: 1978.0 B, free: 366.3 MB)
18/02/27 19:57:41 DEBUG BlockManagerMaster: Updated info of block broadcast_3_piece0
18/02/27 19:57:41 DEBUG BlockManager: Told master about block broadcast_3_piece0
18/02/27 19:57:41 DEBUG BlockManager: Put block broadcast_3_piece0 locally took  1 ms
18/02/27 19:57:41 DEBUG BlockManager: Putting block broadcast_3_piece0 without replication took  1 ms
18/02/27 19:57:41 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
18/02/27 19:57:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (ShuffledRDD[5] at reduceByKey at <console>:26)
18/02/27 19:57:41 DEBUG DAGScheduler: New pending partitions: Set(0, 1)
18/02/27 19:57:41 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
18/02/27 19:57:41 DEBUG TaskSetManager: Epoch for TaskSet 2.0: 1
18/02/27 19:57:41 DEBUG TaskSetManager: Valid locality levels for TaskSet 2.0: ANY
18/02/27 19:57:41 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2.0, runningTasks: 0
18/02/27 19:57:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5, localhost, executor driver, partition 0, ANY, 5750 bytes)
18/02/27 19:57:41 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6, localhost, executor driver, partition 1, ANY, 5750 bytes)
18/02/27 19:57:41 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)
18/02/27 19:57:41 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)
18/02/27 19:57:41 DEBUG Executor: Task 5's epoch is 1
18/02/27 19:57:41 DEBUG BlockManager: Getting local block broadcast_3
18/02/27 19:57:41 DEBUG BlockManager: Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
18/02/27 19:57:41 DEBUG Executor: Task 6's epoch is 1
18/02/27 19:57:41 DEBUG BlockManager: Getting local block broadcast_3
18/02/27 19:57:41 DEBUG BlockManager: Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
18/02/27 19:57:41 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0, partitions 1-2
18/02/27 19:57:41 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0, partitions 0-1
18/02/27 19:57:41 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
18/02/27 19:57:41 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
18/02/27 19:57:41 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/02/27 19:57:41 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/02/27 19:57:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
18/02/27 19:57:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
18/02/27 19:57:42 DEBUG ShuffleBlockFetcherIterator: Got local blocks in  19 ms
18/02/27 19:57:42 DEBUG ShuffleBlockFetcherIterator: Got local blocks in  19 ms
18/02/27 19:57:42 DEBUG TaskMemoryManager: Task 6 release 0.0 B from org.apache.spark.util.collection.ExternalAppendOnlyMap@30dee0e2
18/02/27 19:57:42 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 17433 bytes result sent to driver
18/02/27 19:57:42 DEBUG TaskMemoryManager: Task 5 release 0.0 B from org.apache.spark.util.collection.ExternalAppendOnlyMap@32ccf7ce
18/02/27 19:57:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 17557 bytes result sent to driver
18/02/27 19:57:42 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2.0, runningTasks: 1
18/02/27 19:57:42 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2.0, runningTasks: 0
18/02/27 19:57:42 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 113 ms on localhost (executor driver) (1/2)
18/02/27 19:57:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 121 ms on localhost (executor driver) (2/2)
18/02/27 19:57:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/02/27 19:57:42 INFO DAGScheduler: ResultStage 2 (collect at <console>:29) finished in 0.111 s
18/02/27 19:57:42 DEBUG DAGScheduler: After removal of stage 2, remaining stages = 1
18/02/27 19:57:42 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0
18/02/27 19:57:42 INFO DAGScheduler: Job 1 finished: collect at <console>:29, took 0.658689 s
res1: Array[(String, Int)] = Array((https://t.co/Ncf8ycdpUl,1), (#20Feb,1), (https://t.co/Do0FruHwtS,4), (https://t.co/pTExrF4MXj,1), (https://t.co/imfuDiSzhC,1), (https://t.co/UYFLndUkOA,1), (https://t.co/OTwi4P7t04,1), (https://t.co/w1Kp1iltsH,1), (#PIFFkaScene,1), (https://t.co/tEobiUI90N,1), (https://t.co/XPM2DQek8z,7), (#BARCHE,1), (#Oscars,1), (#conte,1), (#Matches_Today,4), (https://t.co/jOqmpxTjHk,1), (https://t.co/aDya93GmAP,1), (#DRChroniclesBET,3), (#FCB,3), (#FCBBES,1), (#GainWithXtianDela,1), (https://t.co/1aXnaYAxCL,1), (https://t.co/hPAIhW3tVP,1), (https://t.co/dzzNwxl10X,1), (#Dubai,1), (https://t.co/s�,1), (https://t.co/MdHcRzm9KI,1), (https://t.co/NkeF5VawWT,1), (#Florida,3), (https://t.co/Xwauj4TNP5,1), (https://t.co/eTEI4T3MQ2,1), (https://t.co/vwFZegfyAe,1), (https:...
18/02/27 19:59:41 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(2)
18/02/27 19:59:41 DEBUG ContextCleaner: Cleaning broadcast 2
18/02/27 19:59:41 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 2
18/02/27 19:59:41 DEBUG BlockManagerSlaveEndpoint: removing broadcast 2
18/02/27 19:59:41 DEBUG BlockManager: Removing broadcast 2
18/02/27 19:59:41 DEBUG BlockManager: Removing block broadcast_2_piece0
18/02/27 19:59:41 DEBUG MemoryStore: Block broadcast_2_piece0 of size 2822 dropped from memory (free 383817628)
18/02/27 19:59:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.157.131:42721 in memory (size: 2.8 KB, free: 366.3 MB)
18/02/27 19:59:41 DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0
18/02/27 19:59:41 DEBUG BlockManager: Told master about block broadcast_2_piece0
18/02/27 19:59:41 DEBUG BlockManager: Removing block broadcast_2
18/02/27 19:59:41 DEBUG MemoryStore: Block broadcast_2 of size 4920 dropped from memory (free 383822548)
18/02/27 19:59:41 DEBUG BlockManagerSlaveEndpoint: Done removing broadcast 2, response is 0
18/02/27 19:59:41 DEBUG ContextCleaner: Cleaned broadcast 2
18/02/27 19:59:41 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(3)
18/02/27 19:59:41 DEBUG ContextCleaner: Cleaning broadcast 3
18/02/27 19:59:41 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 3
18/02/27 19:59:41 DEBUG BlockManagerSlaveEndpoint: removing broadcast 3
18/02/27 19:59:41 DEBUG BlockManager: Removing broadcast 3
18/02/27 19:59:41 DEBUG BlockManager: Removing block broadcast_3_piece0
18/02/27 19:59:41 DEBUG MemoryStore: Block broadcast_3_piece0 of size 1978 dropped from memory (free 383824526)
18/02/27 19:59:41 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.157.131:42721 in memory (size: 1978.0 B, free: 366.3 MB)
18/02/27 19:59:41 DEBUG BlockManagerMaster: Updated info of block broadcast_3_piece0
18/02/27 19:59:41 DEBUG BlockManager: Told master about block broadcast_3_piece0
18/02/27 19:59:41 DEBUG BlockManager: Removing block broadcast_3
18/02/27 19:59:41 DEBUG MemoryStore: Block broadcast_3 of size 3272 dropped from memory (free 383827798)
18/02/27 19:59:41 DEBUG BlockManagerSlaveEndpoint: Sent response: 0 to 192.168.157.131:39533
18/02/27 19:59:41 DEBUG BlockManagerSlaveEndpoint: Done removing broadcast 3, response is 0
18/02/27 19:59:41 DEBUG ContextCleaner: Cleaned broadcast 3
18/02/27 19:59:41 DEBUG BlockManagerSlaveEndpoint: Sent response: 0 to 192.168.157.131:39533
18/02/27 19:59:41 DEBUG ClosureCleaner: +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1$$anonfun$31) +++
18/02/27 19:59:41 DEBUG ClosureCleaner:  + declared fields: 1
18/02/27 19:59:41 DEBUG ClosureCleaner:      public static final long org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1$$anonfun$31.serialVersionUID
18/02/27 19:59:41 DEBUG ClosureCleaner:  + declared methods: 2
18/02/27 19:59:41 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1$$anonfun$31.apply(java.lang.Object)
18/02/27 19:59:41 DEBUG ClosureCleaner:      public final scala.collection.Iterator org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1$$anonfun$31.apply(scala.collection.Iterator)
18/02/27 19:59:41 DEBUG ClosureCleaner:  + inner classes: 1
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1$$anonfun$31$$anonfun$apply$53
18/02/27 19:59:41 DEBUG ClosureCleaner:  + outer classes: 0
18/02/27 19:59:41 DEBUG ClosureCleaner:  + outer objects: 0
18/02/27 19:59:41 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
18/02/27 19:59:41 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
18/02/27 19:59:41 DEBUG ClosureCleaner:  + there are no enclosing objects!
18/02/27 19:59:41 DEBUG ClosureCleaner:  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1$$anonfun$31) is now cleaned +++
18/02/27 19:59:41 DEBUG PairRDDFunctions: Saving as hadoop file of type (NullWritable, Text)
18/02/27 19:59:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/02/27 19:59:41 DEBUG ClosureCleaner: +++ Cleaning closure <function2> (org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13) +++
18/02/27 19:59:41 DEBUG ClosureCleaner:  + declared fields: 3
18/02/27 19:59:41 DEBUG ClosureCleaner:      public static final long org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.serialVersionUID
18/02/27 19:59:41 DEBUG ClosureCleaner:      private final org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1 org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.$outer
18/02/27 19:59:41 DEBUG ClosureCleaner:      public final org.apache.spark.SparkHadoopWriter org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.writer$2
18/02/27 19:59:41 DEBUG ClosureCleaner:  + declared methods: 3
18/02/27 19:59:41 DEBUG ClosureCleaner:      public org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1 org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.org$apache$spark$rdd$PairRDDFunctions$$anonfun$$anonfun$$$outer()
18/02/27 19:59:41 DEBUG ClosureCleaner:      public final void org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
18/02/27 19:59:41 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(java.lang.Object,java.lang.Object)
18/02/27 19:59:41 DEBUG ClosureCleaner:  + inner classes: 4
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$9
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$58
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$8
18/02/27 19:59:41 DEBUG ClosureCleaner:  + outer classes: 2
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions
18/02/27 19:59:41 DEBUG ClosureCleaner:  + outer objects: 2
18/02/27 19:59:41 DEBUG ClosureCleaner:      <function0>
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions@29c471f6
18/02/27 19:59:41 DEBUG ClosureCleaner:  + populating accessed fields because this is the starting closure
18/02/27 19:59:41 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
18/02/27 19:59:41 DEBUG ClosureCleaner:      (class org.apache.spark.rdd.PairRDDFunctions,Set())
18/02/27 19:59:41 DEBUG ClosureCleaner:      (class org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1,Set($outer))
18/02/27 19:59:41 DEBUG ClosureCleaner:  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.PairRDDFunctions,org.apache.spark.rdd.PairRDDFunctions@29c471f6)
18/02/27 19:59:41 DEBUG ClosureCleaner:  + cloning the object <function0> of class org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1
18/02/27 19:59:41 DEBUG ClosureCleaner:  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1)
18/02/27 19:59:41 DEBUG ClosureCleaner: +++ Cleaning closure <function0> (org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1) +++
18/02/27 19:59:41 DEBUG ClosureCleaner:  + declared fields: 3
18/02/27 19:59:41 DEBUG ClosureCleaner:      public static final long org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.serialVersionUID
18/02/27 19:59:41 DEBUG ClosureCleaner:      private final org.apache.spark.rdd.PairRDDFunctions org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.$outer
18/02/27 19:59:41 DEBUG ClosureCleaner:      private final org.apache.hadoop.mapred.JobConf org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.conf$4
18/02/27 19:59:41 DEBUG ClosureCleaner:  + declared methods: 4
18/02/27 19:59:41 DEBUG ClosureCleaner:      public void org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp()
18/02/27 19:59:41 DEBUG ClosureCleaner:      public org.apache.spark.rdd.PairRDDFunctions org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.org$apache$spark$rdd$PairRDDFunctions$$anonfun$$$outer()
18/02/27 19:59:41 DEBUG ClosureCleaner:      public final java.lang.Object org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply()
18/02/27 19:59:41 DEBUG ClosureCleaner:      public final void org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply()
18/02/27 19:59:41 DEBUG ClosureCleaner:  + inner classes: 6
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$9
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$58
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$apply$mcV$sp$4
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$8
18/02/27 19:59:41 DEBUG ClosureCleaner:  + outer classes: 1
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions
18/02/27 19:59:41 DEBUG ClosureCleaner:  + outer objects: 1
18/02/27 19:59:41 DEBUG ClosureCleaner:      org.apache.spark.rdd.PairRDDFunctions@29c471f6
18/02/27 19:59:41 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
18/02/27 19:59:41 DEBUG ClosureCleaner:      (class org.apache.spark.rdd.PairRDDFunctions,Set())
18/02/27 19:59:41 DEBUG ClosureCleaner:      (class org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1,Set($outer))
18/02/27 19:59:41 DEBUG ClosureCleaner:  + outermost object is not a closure or REPL line object, so do not clone it: (class org.apache.spark.rdd.PairRDDFunctions,org.apache.spark.rdd.PairRDDFunctions@29c471f6)
18/02/27 19:59:41 DEBUG ClosureCleaner:  +++ closure <function0> (org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1) is now cleaned +++
18/02/27 19:59:41 DEBUG ClosureCleaner:  +++ closure <function2> (org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13) is now cleaned +++
18/02/27 19:59:41 INFO SparkContext: Starting job: saveAsTextFile at <console>:29
18/02/27 19:59:41 DEBUG MapOutputTrackerMaster: cached status not found for : 0
18/02/27 19:59:41 DEBUG MapOutputTrackerMaster: cached status not found for : 0
18/02/27 19:59:41 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 163 bytes
18/02/27 19:59:41 INFO DAGScheduler: Got job 2 (saveAsTextFile at <console>:29) with 2 output partitions
18/02/27 19:59:41 INFO DAGScheduler: Final stage: ResultStage 4 (saveAsTextFile at <console>:29)
18/02/27 19:59:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
18/02/27 19:59:41 INFO DAGScheduler: Missing parents: List()
18/02/27 19:59:41 DEBUG DAGScheduler: submitStage(ResultStage 4)
18/02/27 19:59:41 DEBUG DAGScheduler: missing: List()
18/02/27 19:59:41 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[6] at saveAsTextFile at <console>:29), which has no missing parents
18/02/27 19:59:41 DEBUG DAGScheduler: submitMissingTasks(ResultStage 4)
18/02/27 19:59:41 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 72.9 KB, free 366.0 MB)
18/02/27 19:59:41 DEBUG BlockManager: Put block broadcast_4 locally took  3 ms
18/02/27 19:59:41 DEBUG BlockManager: Putting block broadcast_4 without replication took  4 ms
18/02/27 19:59:41 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 26.4 KB, free 365.9 MB)
18/02/27 19:59:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.157.131:42721 (size: 26.4 KB, free: 366.3 MB)
18/02/27 19:59:41 DEBUG BlockManagerMaster: Updated info of block broadcast_4_piece0
18/02/27 19:59:41 DEBUG BlockManager: Told master about block broadcast_4_piece0
18/02/27 19:59:41 DEBUG BlockManager: Put block broadcast_4_piece0 locally took  1 ms
18/02/27 19:59:41 DEBUG BlockManager: Putting block broadcast_4_piece0 without replication took  1 ms
18/02/27 19:59:41 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:996
18/02/27 19:59:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[6] at saveAsTextFile at <console>:29)
18/02/27 19:59:41 DEBUG DAGScheduler: New pending partitions: Set(0, 1)
18/02/27 19:59:41 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
18/02/27 19:59:41 DEBUG TaskSetManager: Epoch for TaskSet 4.0: 1
18/02/27 19:59:41 DEBUG TaskSetManager: Valid locality levels for TaskSet 4.0: ANY
18/02/27 19:59:41 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_4.0, runningTasks: 0
18/02/27 19:59:41 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7, localhost, executor driver, partition 0, ANY, 5758 bytes)
18/02/27 19:59:41 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 8, localhost, executor driver, partition 1, ANY, 5758 bytes)
18/02/27 19:59:41 INFO Executor: Running task 0.0 in stage 4.0 (TID 7)
18/02/27 19:59:41 DEBUG Executor: Task 7's epoch is 1
18/02/27 19:59:41 DEBUG BlockManager: Getting local block broadcast_4
18/02/27 19:59:41 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
18/02/27 19:59:41 INFO Executor: Running task 1.0 in stage 4.0 (TID 8)
18/02/27 19:59:41 DEBUG Executor: Task 8's epoch is 1
18/02/27 19:59:41 DEBUG BlockManager: Getting local block broadcast_4
18/02/27 19:59:41 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
18/02/27 19:59:41 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0, partitions 0-1
18/02/27 19:59:41 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
18/02/27 19:59:41 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/02/27 19:59:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
18/02/27 19:59:41 DEBUG ShuffleBlockFetcherIterator: Got local blocks in  0 ms
18/02/27 19:59:41 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0, partitions 1-2
18/02/27 19:59:41 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
18/02/27 19:59:41 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/02/27 19:59:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
18/02/27 19:59:41 DEBUG ShuffleBlockFetcherIterator: Got local blocks in  0 ms
18/02/27 19:59:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/02/27 19:59:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/02/27 19:59:42 DEBUG TaskMemoryManager: Task 7 release 0.0 B from org.apache.spark.util.collection.ExternalAppendOnlyMap@2d8007e4
18/02/27 19:59:42 DEBUG TaskMemoryManager: Task 8 release 0.0 B from org.apache.spark.util.collection.ExternalAppendOnlyMap@5cf1db85
18/02/27 19:59:42 DEBUG OutputCommitCoordinator: Authorizing attemptNumber=0 to commit for stage=4, partition=0
18/02/27 19:59:42 INFO FileOutputCommitter: Saved output of task 'attempt_20180227195941_0004_m_000000_7' to file:/home/kopps/Desktop/output_spark/_temporary/0/task_20180227195941_0004_m_000000
18/02/27 19:59:42 INFO SparkHadoopMapRedUtil: attempt_20180227195941_0004_m_000000_7: Committed
18/02/27 19:59:42 DEBUG OutputCommitCoordinator: Authorizing attemptNumber=0 to commit for stage=4, partition=1
18/02/27 19:59:42 INFO FileOutputCommitter: Saved output of task 'attempt_20180227195941_0004_m_000001_8' to file:/home/kopps/Desktop/output_spark/_temporary/0/task_20180227195941_0004_m_000001
18/02/27 19:59:42 INFO SparkHadoopMapRedUtil: attempt_20180227195941_0004_m_000001_8: Committed
18/02/27 19:59:42 INFO Executor: Finished task 0.0 in stage 4.0 (TID 7). 1977 bytes result sent to driver
18/02/27 19:59:42 INFO Executor: Finished task 1.0 in stage 4.0 (TID 8). 1890 bytes result sent to driver
18/02/27 19:59:42 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_4.0, runningTasks: 1
18/02/27 19:59:42 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_4.0, runningTasks: 0
18/02/27 19:59:42 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 218 ms on localhost (executor driver) (1/2)
18/02/27 19:59:42 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 8) in 217 ms on localhost (executor driver) (2/2)
18/02/27 19:59:42 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
18/02/27 19:59:42 INFO DAGScheduler: ResultStage 4 (saveAsTextFile at <console>:29) finished in 0.219 s
18/02/27 19:59:42 DEBUG DAGScheduler: After removal of stage 4, remaining stages = 1
18/02/27 19:59:42 DEBUG DAGScheduler: After removal of stage 3, remaining stages = 0
18/02/27 19:59:42 INFO DAGScheduler: Job 2 finished: saveAsTextFile at <console>:29, took 0.243164 s
18/02/27 19:59:42 DEBUG FileOutputCommitter: Merging data from DeprecatedRawLocalFileStatus{path=file:/home/kopps/Desktop/output_spark/_temporary/0/task_20180227195941_0004_m_000001; isDirectory=true; modification_time=1519790382000; access_time=0; owner=; group=; permission=rwxrwxrwx; isSymlink=false} to file:/home/kopps/Desktop/output_spark
18/02/27 19:59:42 DEBUG FileOutputCommitter: Merging data from DeprecatedRawLocalFileStatus{path=file:/home/kopps/Desktop/output_spark/_temporary/0/task_20180227195941_0004_m_000001/part-00001; isDirectory=false; length=11044; replication=1; blocksize=33554432; modification_time=1519790382000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/home/kopps/Desktop/output_spark/part-00001
18/02/27 19:59:42 DEBUG FileOutputCommitter: Merging data from DeprecatedRawLocalFileStatus{path=file:/home/kopps/Desktop/output_spark/_temporary/0/task_20180227195941_0004_m_000000; isDirectory=true; modification_time=1519790382000; access_time=0; owner=; group=; permission=rwxrwxrwx; isSymlink=false} to file:/home/kopps/Desktop/output_spark
18/02/27 19:59:42 DEBUG FileOutputCommitter: Merging data from DeprecatedRawLocalFileStatus{path=file:/home/kopps/Desktop/output_spark/_temporary/0/task_20180227195941_0004_m_000000/part-00000; isDirectory=false; length=11017; replication=1; blocksize=33554432; modification_time=1519790382000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/home/kopps/Desktop/output_spark/part-00000
